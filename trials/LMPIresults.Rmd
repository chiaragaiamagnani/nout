---
title: "Comparison between different local tests: Simes, Simes with Storey and Wilcoxon-Mann-Whitney"
output: pdf_document
date: "20-04-2023"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


The aim is to compare the performance of three closed testing procedures, which respectively use Simes local test with and without Storey estimator for the proportion of true null hypotheses and Wilcoxon-Mann-Whitney local test.
<!-- The aim is to verify if using Wilcoxon-Mann-Whitney test as local test makes the closed testing procedure Locally Most Powerful among Invariant tests (LMPI). -->

We consider a null distribution $F$ from which inliers come and we consider outliers which come from the alternative distribution $$F^k$$ with $k>0$, especially if $k\in\mathbb{N}_{>0}$ we know that $F^k$ is the distribution of the random variable defined as the maximum of $k$ observations drawn from $F$. So, we consider a sample drawn from the mixture distribution $$G = (1-\theta) F+\theta F^k$$ where $\theta\in[0,1]$ is the proportion of inliers.

Since we deal with conformal $p$-values, we are interested not really in the sample distribution, but rather in the scores sample distribution.
In our simulation study we draw $n$ observations for the train set, $l$ for the calibration set and $mk$ for the test set. All observations are drawn from a $d$-multivariate standard normal distribution with $d=3$ and using the algorithm of ***isolation forest*** trained on the training samples we compute the scores for the calibration and test samples. In order to generate scores associated to outlier observations, we consider the $m$ blocks of $k$ observations which create the test set. For each block $i$ with $i=1,\ldots,m$, we draw from a Bernoulli random variable with success probability equal to $\theta$. If the value is 1 then the $i$-th observation of the test set is inlier and we randomly sample its score value from the $k$ scores of the $i$-th block, otherwise it is an outlier and its score value will be the maximum of the scores of the $i$-th block.


<!-- ### Simes closed testing -->


<!-- ```{r eval=F} -->




<!-- ``` -->




<!-- ### StoreySimes Closed Testing -->

<!-- ```{r eval=F} -->




<!-- ``` -->




<!-- ### Wilcoxon-Mann-Whitney Closed Testing -->


<!-- ```{r eval=F} -->




<!-- ``` -->






```{r}

library(mvtnorm)
library(nout)
library(isotree)



d_benjhoch = function(S_Y, S_X, alpha = 0.1){
  m = length(S_Y)
  n = length(S_X)
  pval = sapply(1:m, function(i) (1+sum(S_X >= S_Y[i]))/(n+1))
  d =  sum(stats::p.adjust(pval,"BH")<=alpha)
  return(d)
}






d_StoreyBH = function(S_Y, S_X, alpha = 0.1, lambda=0.5){
  m = length(S_Y)
  n = length(S_X)
  pval = sort(sapply(1:m, function(i) (1+sum(S_X >= S_Y[i]))/(n+1)), decreasing=FALSE)
  pi0Sto = sapply(1:m, function(i) (1+sum(pval>lambda))/(m*(1-lambda)))
  d =  sum(stats::p.adjust(pval,"BH")<=alpha/pi0Sto)
  return(d)
}





scores_from_mixture = function(k, raw_scores, theta){

  if(theta>1 || theta<0){
    stop("Error: argument theta should in [0,1] interval")
  }

  ll = length(raw_scores)
  if(ll<k){
    stop("Error: length of raw_scores is smaller than k.")
  }

  quotient = ll%/%k
  remainder = ll%%k

  if(remainder != 0){
    cat("Warning: length of raw_scores is not a multiple of k. Last ",
        remainder, "elements of raw_scores will not be used.")
  }

  usable.raw_scores = raw_scores[1:(ll-remainder)]
  success = replicate(quotient, rbinom(1,1,theta))

  scores = rep(0, times = quotient)
  outlier = rep(0, times = quotient)

  for(i in 0:(quotient-1)){
    # if TRUE draw from the alternative distribution
    if(success[i+1]==T){
      scores[i+1] = max(usable.raw_scores[(i*k+1):(i*k+k)])
      outlier[i+1]=T
    }
    # if FALSE draw from the null distribution
    if(success[i+1]==F){
      scores[i+1] = sample(usable.raw_scores[(i*k+1):(i*k+k)], size=1)
    }
  }

  return(list("scores"=scores, "outlier"=outlier))
}






simuLMPI = function(B=10^4, n, l, m, d = 3, k = 2, theta, alpha = m/(n+1)){

  train = mvtnorm::rmvnorm(n=n, mean=rep(0,d))
  iso.fo = isotree::isolation.forest(train, ndim=d, ntrees=10, nthreads=1, 
                                     scoring_metric = "depth", output_score = TRUE)

  crit=critWMW(m=m, n=n, alpha=alpha)

  d_WMW = rep(0,B)
  d_Simes = rep(0,B)
  d_StoSimes = rep(0,B)
  d_BH = rep(0,B)
  d_StoBH = rep(0,B)

  for(b in 1:B){
    cal = mvtnorm::rmvnorm(n=l, mean=rep(0,d))
    te = mvtnorm::rmvnorm(n=k*m, mean=rep(0,d))

    S_cal = isotree::predict.isolation_forest(iso.fo$model, cal, type = "score")
    rawS_te = isotree::predict.isolation_forest(iso.fo$model, te, type = "score")
    gen.te.score = scores_from_mixture(k=k, raw_scores=rawS_te, theta=theta)
    S_te = gen.te.score$scores

    d_WMW[b] = d_mannwhitney(S_X=S_cal, S_Y=S_te, crit=crit)
    d_Simes[b] = d_Simes(S_X=S_cal, S_Y=S_te, alpha=alpha)
    d_StoSimes[b] = d_StoreySimes(S_X=S_cal, S_Y=S_te, alpha=alpha)
    d_BH[b] = d_benjhoch(S_X=S_cal, S_Y=S_te, alpha=alpha)
    d_StoBH[b] = d_StoreyBH(S_X=S_cal, S_Y=S_te, alpha=alpha)
  }

  discov = as.data.frame(cbind("d_BH"=d_BH>0, "d_StoBH"=d_StoBH>0, "d_Simes"=d_Simes>0,
                               "d_StoSimes"=d_StoSimes>0, "d_WMW"=d_WMW>0))
  colnames(discov) = c("BH", "BHSto", "CTSim", "CTSimSto", "CTWMW")
  res = apply(discov, MARGIN = 2, FUN = mean)

  return(list("results"=res, "discoveries"=discov, "theta"=theta, "alpha"=alpha))
}



```







### K=2

```{r}
set.seed(321)

# Initializing parameters
B=10^4
n = 19
l = 19
m = 2
d = 3
k = 2
alpha = m/(n+1)
thetas = c(0, 0.01, 0.03, 0.05, 0.07, 0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 0.99, 1)

# Results
res = lapply(thetas, function(theta) simuLMPI(B=B, n=n, l=l, m=m, d = d, 
                                              k = k, theta, alpha = m/(n+1)))

# Storing results
store_res = matrix(nrow=length(thetas), ncol = 5)
row.names = rep(NA, times=length(thetas))
for(i in 1:length(thetas)){
  row.names[i] = paste("theta =",thetas[i])
}
rownames(store_res) = row.names  
colnames(store_res) = c("BH", "StoBH", "Simes", "StoSimes", "WMW")

for(i in 1:length(res)){
  store_res[i,] = res[[i]]$results
}

store_res




```




### K=3

```{r}
set.seed(321)

# Initializing parameters
B=10^4
n = 19
l = 19
m = 2
d = 3
k = 3
alpha = m/(n+1)
thetas = c(0, 0.01, 0.03, 0.05, 0.07, 0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 0.99, 1)

# Results
res = lapply(thetas, function(theta) simuLMPI(B=B, n=n, l=l, m=m, d = d, 
                                              k = k, theta, alpha = m/(n+1)))

# Storing results
store_res = matrix(nrow=length(thetas), ncol = 5)
row.names = rep(NA, times=length(thetas))
for(i in 1:length(thetas)){
  row.names[i] = paste("theta =",thetas[i])
}
rownames(store_res) = row.names  
colnames(store_res) = c("BH", "StoBH", "Simes", "StoSimes", "WMW")

for(i in 1:length(res)){
  store_res[i,] = res[[i]]$results
}

store_res




```






### K=5

```{r}
set.seed(321)

# Initializing parameters
B=10^4
n = 19
l = 19
m = 2
d = 3
k = 5
alpha = m/(n+1)
thetas = c(0, 0.01, 0.03, 0.05, 0.07, 0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 0.99, 1)

# Results
res = lapply(thetas, function(theta) simuLMPI(B=B, n=n, l=l, m=m, d = d, 
                                              k = k, theta, alpha = m/(n+1)))

# Storing results
store_res = matrix(nrow=length(thetas), ncol = 5)
row.names = rep(NA, times=length(thetas))
for(i in 1:length(thetas)){
  row.names[i] = paste("theta =",thetas[i])
}
rownames(store_res) = row.names  
colnames(store_res) = c("BH", "StoBH", "Simes", "StoSimes", "WMW")

for(i in 1:length(res)){
  store_res[i,] = res[[i]]$results
}

store_res




```






### K=10

```{r}
set.seed(321)

# Initializing parameters
B=10^4
n = 19
l = 19
m = 2
d = 3
k = 10
alpha = m/(n+1)
thetas = c(0, 0.01, 0.03, 0.05, 0.07, 0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 0.99, 1)

# Results
res = lapply(thetas, function(theta) simuLMPI(B=B, n=n, l=l, m=m, d = d, 
                                              k = k, theta, alpha = m/(n+1)))

# Storing results
store_res = matrix(nrow=length(thetas), ncol = 5)
row.names = rep(NA, times=length(thetas))
for(i in 1:length(thetas)){
  row.names[i] = paste("theta =",thetas[i])
}
rownames(store_res) = row.names  
colnames(store_res) = c("BH", "StoBH", "Simes", "StoSimes", "WMW")

for(i in 1:length(res)){
  store_res[i,] = res[[i]]$results
}

store_res




```










### K=15

```{r}
set.seed(321)

# Initializing parameters
B=10^3
n = 19
l = 19
m = 2
d = 3
k = 15
alpha = m/(n+1)
thetas = c(0, 0.01, 0.03, 0.05, 0.07, 0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 0.99, 1)

# Results
res = lapply(thetas, function(theta) simuLMPI(B=B, n=n, l=l, m=m, d = d, 
                                              k = k, theta, alpha = m/(n+1)))

# Storing results
store_res = matrix(nrow=length(thetas), ncol = 5) 
row.names = rep(NA, times=length(thetas))
for(i in 1:length(thetas)){
  row.names[i] = paste("theta =",thetas[i])
}
rownames(store_res) = row.names  
colnames(store_res) = c("BH", "StoBH", "Simes", "StoSimes", "WMW")

for(i in 1:length(res)){
  store_res[i,] = res[[i]]$results
}

store_res




```















